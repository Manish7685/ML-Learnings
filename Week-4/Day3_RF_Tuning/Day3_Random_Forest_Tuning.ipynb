{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb1bb02-7b09-4b9f-b827-04291a2c413f",
   "metadata": {},
   "source": [
    "# Day 3 — Random Forest Hyperparameter Tuning\n",
    "### Machine Learning Roadmap — Week 4\n",
    "### Author — N Manish Kumar\n",
    "---\n",
    "\n",
    "Random Forests are strong baseline models, but their performance and\n",
    "generalization depend heavily on hyperparameters such as:\n",
    "\n",
    "- Number of trees\n",
    "- Tree depth\n",
    "- Minimum samples per split\n",
    "- Number of features considered at each split\n",
    "\n",
    "These parameters control the bias–variance trade-off.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Train a default Random Forest model\n",
    "- Tune key hyperparameters using cross-validation\n",
    "- Compare tuned vs default performance\n",
    "- Understand how tuning affects bias and variance\n",
    "\n",
    "Dataset used: **Breast Cancer Dataset (sklearn)**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dataset Loading and Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f218fa-95c5-477b-8671-3cfcd2b6cbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (455, 30)\n",
      "Test shape: (114, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c70e2c-a508-4559-96a6-6a118172c0b1",
   "metadata": {},
   "source": [
    "## 2. Default Random Forest Baseline\n",
    "\n",
    "Before tuning hyperparameters, we first train a Random Forest using default\n",
    "settings provided by scikit-learn.\n",
    "\n",
    "This baseline performance will be used to compare against the tuned model\n",
    "to determine whether hyperparameter optimization provides real improvement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5f7368-826e-41e3-9ea3-41339af0724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default RF Train Accuracy: 1.0\n",
      "Default RF Test Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "# Default Random Forest\n",
    "rf_default = RandomForestClassifier(random_state=42, n_jobs = -1)\n",
    "rf_default.fit(X_train,y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_acc_default = accuracy_score(y_train, rf_default.predict(X_train))\n",
    "test_acc_default = accuracy_score(y_test, rf_default.predict(X_test))\n",
    "\n",
    "print(\"Default RF Train Accuracy:\", train_acc_default)\n",
    "print(\"Default RF Test Accuracy:\", test_acc_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba056c-73b4-4de3-81ac-119a12b20920",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Training accuracy is usually very high for Random Forests, indicating strong\n",
    "ability to fit training data.\n",
    "\n",
    "Test accuracy reflects how well the model generalizes to unseen samples.\n",
    "\n",
    "This baseline result will be compared with the tuned Random Forest to evaluate\n",
    "whether hyperparameter optimization improves generalization.\n",
    "\n",
    "---\n",
    "## 3. Hyperparameter Grid and GridSearchCV\n",
    "\n",
    "Random Forest performance depends on several key hyperparameters that control\n",
    "model complexity and randomness.\n",
    "\n",
    "Important parameters include:\n",
    "- n_estimators: number of trees in the forest\n",
    "- max_depth: maximum depth of each tree\n",
    "- min_samples_split: minimum samples needed to split a node\n",
    "- max_features: number of features considered at each split\n",
    "\n",
    "We use GridSearchCV to:\n",
    "- Try multiple combinations of these parameters\n",
    "- Evaluate each using cross-validation\n",
    "- Select the combination that gives best average performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1431bc-f46a-45bb-aadd-aeac1df6304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best CV Accuracy: 0.9604395604395606\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100,200],\n",
    "    \"max_depth\": [None,5,10],\n",
    "    \"min_samples_split\": [2,5],\n",
    "    \"max_features\": [\"sqrt\",\"log2\"]\n",
    "}\n",
    "rf = RandomForestClassifier(random_state = 42, n_jobs = -1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf,\n",
    "    param_grid = param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs =-1,\n",
    "    verbose = 1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777a052d-3294-4ec7-b1be-f70818a4423f",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "GridSearchCV evaluates many Random Forest configurations using cross-validation\n",
    "and selects the parameter combination that achieves the highest average\n",
    "validation accuracy.\n",
    "\n",
    "This process helps find a better bias–variance balance than default settings\n",
    "and reduces the risk of choosing parameters that only work well on one split.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Comparing Tuned Random Forest with Default Model\n",
    "\n",
    "After finding the best hyperparameters using cross-validation, we now evaluate\n",
    "the tuned model on the test set.\n",
    "\n",
    "We compare:\n",
    "- Training accuracy\n",
    "- Test accuracy\n",
    "\n",
    "between the default Random Forest and the tuned Random Forest to determine\n",
    "whether tuning improved generalization or only fit the training data better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "035acf18-a4f9-4784-8f1f-a7dbb9649b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default RF -> Train: 1.0 Test: 0.956140350877193\n",
      "Tuned RF   -> Train: 1.0 Test: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "# Best tuned rf model from grid search\n",
    "rf_tuned = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Tuned Model\n",
    "train_acc_tuned = accuracy_score(y_train, rf_tuned.predict(X_train))\n",
    "test_acc_tuned = accuracy_score(y_test, rf_tuned.predict(X_test))\n",
    "\n",
    "print(\"Default RF -> Train:\", train_acc_default, \"Test:\", test_acc_default)\n",
    "print(\"Tuned RF   -> Train:\", train_acc_tuned, \"Test:\", test_acc_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd60a6-f684-436b-a44f-e3b860606980",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "If test accuracy improves while training accuracy does not increase\n",
    "significantly, tuning has improved generalization.\n",
    "\n",
    "If training accuracy increases but test accuracy does not, tuning may have\n",
    "led to overfitting.\n",
    "\n",
    "The best tuning outcome is higher or similar training accuracy combined with\n",
    "improved test performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Understanding the Effect of Hyperparameters\n",
    "\n",
    "Hyperparameter tuning is most useful when we understand *why* certain settings\n",
    "perform better.\n",
    "\n",
    "Key Random Forest parameters affect the model in different ways:\n",
    "\n",
    "- n_estimators:\n",
    "  More trees reduce variance but increase training time.\n",
    "\n",
    "- max_depth:\n",
    "  Controls how complex each tree is.\n",
    "  Shallow trees → higher bias, deeper trees → higher variance.\n",
    "\n",
    "- min_samples_split:\n",
    "  Prevents trees from creating very small, highly specific splits.\n",
    "\n",
    "- max_features:\n",
    "  Controls randomness at each split.\n",
    "  Smaller values increase diversity among trees and reduce correlation.\n",
    "\n",
    "By examining the best parameters found by GridSearch, we can infer how the\n",
    "model balanced bias and variance on this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "998555aa-17c2-4e94-ad2a-ea7eef5da50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_max_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.960440</td>\n",
       "      <td>200</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.960440</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>sqrt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.956044</td>\n",
       "      <td>200</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>log2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.956044</td>\n",
       "      <td>100</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.956044</td>\n",
       "      <td>200</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>log2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  param_n_estimators param_max_depth  \\\n",
       "1          0.960440                 200            None   \n",
       "17         0.960440                 200              10   \n",
       "5          0.956044                 200            None   \n",
       "6          0.956044                 100            None   \n",
       "7          0.956044                 200            None   \n",
       "\n",
       "    param_min_samples_split param_max_features  \n",
       "1                         2               sqrt  \n",
       "17                        2               sqrt  \n",
       "5                         2               log2  \n",
       "6                         5               log2  \n",
       "7                         5               log2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert grid search results to DataFrame\n",
    "cv_results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Show top 5 configurations\n",
    "cv_results_df[\n",
    "    [\"mean_test_score\", \"param_n_estimators\", \"param_max_depth\",\n",
    "     \"param_min_samples_split\", \"param_max_features\"]\n",
    "].sort_values(by = \"mean_test_score\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e56a23-c6b1-44c3-954a-335b4fe923fa",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The best-performing configurations usually balance tree depth and randomness.\n",
    "\n",
    "If shallow depths and higher min_samples_split are selected, it suggests that\n",
    "reducing overfitting was important.\n",
    "\n",
    "If deeper trees perform better, it suggests that the dataset benefits from\n",
    "more complex decision boundaries.\n",
    "\n",
    "Understanding these trends helps guide future tuning instead of relying\n",
    "entirely on brute-force search.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc7bcf-a71b-4083-8845-3684f3fe57aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
