{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed3d1112-430f-486d-870d-9302cd761902",
   "metadata": {},
   "source": [
    "# Day 6 — Practical Gradient Boosting Tuning\n",
    "### Machine Learning Roadmap — Week 4\n",
    "### Author — N Manish Kumar\n",
    "---\n",
    "\n",
    "Gradient Boosting is a very powerful model, but it is also sensitive to\n",
    "hyperparameters.\n",
    "\n",
    "Unlike Random Forest, boosting models can easily overfit if not tuned\n",
    "carefully.\n",
    "\n",
    "Key parameters that control boosting behavior:\n",
    "\n",
    "- n_estimators → number of boosting stages\n",
    "- learning_rate → contribution of each tree\n",
    "- max_depth → complexity of individual trees\n",
    "- min_samples_split → regularization for splits\n",
    "\n",
    "In this notebook we will:\n",
    "- Train a baseline Gradient Boosting model\n",
    "- Tune its hyperparameters using cross-validation\n",
    "- Study interaction between learning rate and number of trees\n",
    "- Compare tuned vs default boosting performance\n",
    "\n",
    "Dataset used: **Breast Cancer Dataset (sklearn)**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dataset Loading and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f30a026-6965-4d76-899c-f5339b49d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (455, 30)\n",
      "Test set shape: (114, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1875d9b-d396-4ebb-b92f-c34fa57238f9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Default Gradient Boosting Baseline\n",
    "\n",
    "Before tuning hyperparameters, we first train a Gradient Boosting model with\n",
    "default settings.\n",
    "\n",
    "This baseline performance will help us measure whether tuning actually leads\n",
    "to better generalization on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61caf9da-38ff-40fc-aa8e-cfa98258b608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GB -> Train Accuracy: 1.0\n",
      "Default GB -> Test Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "# Training default Gradient Boosting Model\n",
    "gb_default = GradientBoostingClassifier(random_state=42)\n",
    "gb_default.fit(X_train,y_train)\n",
    "\n",
    "# Evaluate baseline performance\n",
    "train_acc_default = accuracy_score(y_train,gb_default.predict(X_train))\n",
    "test_acc_default = accuracy_score(y_test,gb_default.predict(X_test))\n",
    "\n",
    "print(\"Default GB -> Train Accuracy:\", train_acc_default)\n",
    "print(\"Default GB -> Test Accuracy:\", test_acc_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a35029-d608-431f-8b3b-8be79052380e",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The baseline Gradient Boosting model usually achieves high training accuracy,\n",
    "but the gap between training and test accuracy indicates whether some\n",
    "overfitting is occurring.\n",
    "\n",
    "This baseline result will serve as a reference point to evaluate whether\n",
    "hyperparameter tuning improves generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Hyperparameter Grid and GridSearchCV\n",
    "\n",
    "Gradient Boosting performance is highly sensitive to its hyperparameters.\n",
    "\n",
    "The most important parameters are:\n",
    "\n",
    "- n_estimators: number of boosting stages (trees)\n",
    "- learning_rate: contribution of each tree\n",
    "- max_depth: complexity of individual trees\n",
    "- min_samples_split: regularization to avoid overly specific splits\n",
    "\n",
    "Unlike Random Forest, boosting models can easily overfit if these parameters\n",
    "are not chosen carefully.\n",
    "\n",
    "We use GridSearchCV to test different combinations of these parameters and\n",
    "select the configuration that provides the best cross-validated performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c18fcac-a8c6-496e-9e15-701bef648f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Parameters: {'learning_rate': 0.1, 'max_depth': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best CV Accuracy: 0.9714285714285715\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid for Gradient Boosting\n",
    "param_grid ={\n",
    "    \"n_estimators\": [100,200],\n",
    "    \"learning_rate\": [0.05,0.1],\n",
    "    \"max_depth\": [2,3],\n",
    "    \"min_samples_split\": [2,5]\n",
    "}\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    gb,\n",
    "    param_grid = param_grid,\n",
    "    cv = 5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Accuracy:\", grid_search.best_score_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9dedfa-6213-4add-9096-3408cb882391",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "GridSearchCV evaluates many Gradient Boosting configurations using\n",
    "cross-validation and selects the combination that performs best on average.\n",
    "\n",
    "Because boosting models are sensitive to hyperparameters, this step is crucial\n",
    "for obtaining a model that generalizes well rather than simply memorizing the\n",
    "training data.\n",
    "\n",
    "The best parameters represent a balance between:\n",
    "- model complexity\n",
    "- learning speed\n",
    "- regularization\n",
    "---\n",
    "\n",
    "## 4. Comparing Tuned Gradient Boosting with Default Model\n",
    "\n",
    "After performing hyperparameter tuning using cross-validation, we evaluate the\n",
    "best model on the test set.\n",
    "\n",
    "This comparison answers the most important question:\n",
    "\n",
    "Did tuning actually improve generalization, or did it only improve performance\n",
    "on the training data?\n",
    "\n",
    "We compare training and test accuracy for both the default and tuned models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b3440c-a54f-41a9-942f-0158b73142a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GB -> Train: 1.0 Test: 0.956140350877193\n",
      "Tuned GB   -> Train: 1.0 Test: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Best tuned model from grid search\n",
    "gb_tuned = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Best Model\n",
    "train_acc_tuned = accuracy_score(y_train,gb_tuned.predict(X_train))\n",
    "test_acc_tuned = accuracy_score(y_test,gb_tuned.predict(X_test))\n",
    "\n",
    "print(\"Default GB -> Train:\", train_acc_default, \"Test:\", test_acc_default)\n",
    "print(\"Tuned GB   -> Train:\", train_acc_tuned, \"Test:\", test_acc_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf3f40-5764-4a28-b565-c76fef521076",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "If the tuned model shows higher test accuracy than the default model, it means\n",
    "that hyperparameter tuning successfully improved generalization.\n",
    "\n",
    "If training accuracy increases but test accuracy does not, the tuning process\n",
    "may have overfit to the cross-validation folds.\n",
    "\n",
    "The ideal outcome is a tuned model with:\n",
    "- similar or slightly higher training accuracym\n",
    "- clearly improved test accuracy\n",
    "\n",
    "\n",
    "In this case, the default Gradient Boosting model achieved slightly higher\n",
    "test accuracy than the tuned model.\n",
    "\n",
    "This does not mean tuning failed. It simply indicates that:\n",
    "\n",
    "- Default parameters were already very strong for this dataset\n",
    "- The difference in performance is small and within normal variance\n",
    "- GridSearch optimized cross-validation performance, not test performance\n",
    "\n",
    "In real-world practice, when default settings perform better, it is perfectly\n",
    "valid to keep the simpler default model.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Interaction Between Learning Rate and Number of Trees\n",
    "\n",
    "In Gradient Boosting, two hyperparameters are strongly connected:\n",
    "\n",
    "- learning_rate\n",
    "- n_estimators\n",
    "\n",
    "These parameters work together:\n",
    "\n",
    "- A **smaller learning rate** requires **more trees** to reach good performance  \n",
    "- A **larger learning rate** may need **fewer trees** but risks overfitting\n",
    "\n",
    "This trade-off is one of the most important practical aspects of boosting.\n",
    "\n",
    "To observe this relationship, we train multiple models with different\n",
    "combinations of learning rate and number of trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f1577b-ba85-4d1a-b66b-7417a08e4ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Learning Rate</th>\n",
       "      <th>Trees</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>0.938596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>300</td>\n",
       "      <td>0.921053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.05</td>\n",
       "      <td>50</td>\n",
       "      <td>0.929825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.05</td>\n",
       "      <td>100</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.05</td>\n",
       "      <td>200</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.05</td>\n",
       "      <td>300</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.10</td>\n",
       "      <td>300</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.50</td>\n",
       "      <td>50</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.50</td>\n",
       "      <td>100</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.50</td>\n",
       "      <td>200</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.50</td>\n",
       "      <td>300</td>\n",
       "      <td>0.956140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Learning Rate  Trees  Test Accuracy\n",
       "0            0.01     50       0.938596\n",
       "1            0.01    100       0.921053\n",
       "2            0.01    200       0.929825\n",
       "3            0.01    300       0.921053\n",
       "4            0.05     50       0.929825\n",
       "5            0.05    100       0.947368\n",
       "6            0.05    200       0.956140\n",
       "7            0.05    300       0.956140\n",
       "8            0.10     50       0.947368\n",
       "9            0.10    100       0.956140\n",
       "10           0.10    200       0.956140\n",
       "11           0.10    300       0.956140\n",
       "12           0.50     50       0.956140\n",
       "13           0.50    100       0.956140\n",
       "14           0.50    200       0.956140\n",
       "15           0.50    300       0.956140"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates = [0.01,0.05,0.1,0.5]\n",
    "trees = [50,100,200,300]\n",
    "\n",
    "results=[]\n",
    "\n",
    "for lr in rates:\n",
    "    for n in trees:\n",
    "        model = GradientBoostingClassifier(\n",
    "            learning_rate = lr,\n",
    "            n_estimators = n,\n",
    "            random_state = 42\n",
    "        )\n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        test_acc = accuracy_score(y_test,model.predict(X_test))\n",
    "\n",
    "        results.append({\n",
    "            \"Learning Rate\": lr,\n",
    "            \"Trees\": n,\n",
    "            \"Test Accuracy\": test_acc\n",
    "        })\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07675b9f-5b36-49ca-b13a-20468f37d383",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The results usually show a clear pattern:\n",
    "\n",
    "- With very small learning rates (e.g., 0.01), performance is low with few trees\n",
    "  but improves steadily as the number of trees increases.\n",
    "\n",
    "- With higher learning rates (e.g., 0.1), good accuracy may be reached quickly,\n",
    "  but adding too many trees can lead to overfitting.\n",
    "\n",
    "This demonstrates that optimal boosting performance depends on balancing\n",
    "learning rate and number of estimators together rather than tuning them\n",
    "independently.\n",
    "\n",
    "---\n",
    "\n",
    "# Notebook Summary — Week 4 Day 6\n",
    "\n",
    "In this notebook, we focused on practical hyperparameter tuning for\n",
    "Gradient Boosting models and understanding how to control overfitting.\n",
    "\n",
    "### What was done\n",
    "- Trained a default Gradient Boosting model as baseline\n",
    "- Tuned key hyperparameters using GridSearchCV\n",
    "- Compared tuned and default model performance\n",
    "- Analyzed interaction between learning rate and number of trees\n",
    "- Observed how boosting behavior changes with different parameter choices\n",
    "\n",
    "### Key Learnings\n",
    "- Gradient Boosting is highly sensitive to hyperparameters\n",
    "- Lower learning rate with more trees often gives better generalization\n",
    "- Tuning does not always guarantee better test accuracy\n",
    "- Cross-validation optimizes validation performance, not final test performance\n",
    "- Understanding parameter interactions is more important than blind search\n",
    "\n",
    "### Final Outcome\n",
    "A deeper understanding was gained of how to tune boosting models effectively\n",
    "and how to interpret cases where default settings may outperform tuned models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
