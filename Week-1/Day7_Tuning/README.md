# ğŸŒŸ Day 7 â€” Hyperparameter Tuning (Random Forest Optimization)

This notebook focuses on improving the Random Forest classifier using **RandomizedSearchCV** and evaluating the optimized model using metrics such as accuracy, ROC curve, and AUC. Hyperparameter tuning is a key step for improving generalization and model performance.

---

## ğŸ“Œ Objectives

- Perform hyperparameter tuning using RandomizedSearchCV  
- Identify the best-performing Random Forest model  
- Evaluate tuned vs baseline performance  
- Plot and compare ROC curves  
- Save the final optimized model for future use  

---

## ğŸ§  Key Concepts Learned

### ğŸ”¹ Hyperparameter Tuning
Hyperparameters control a modelâ€™s behavior.  
RandomizedSearchCV helps explore many combinations efficiently using cross-validation.

### ğŸ”¹ Cross-Validation (CV)
Used to measure model stability and prevent overfitting.

### ğŸ”¹ ROC Curve & AUC
AUC measures class separation quality.  
Higher AUC = better performance across thresholds.

---

## ğŸ› ï¸ Steps Completed

### **1ï¸âƒ£ Loaded Processed Dataset**
Imported the feature-engineered Titanic dataset prepared on Day 4.

---

### **2ï¸âƒ£ Baseline Random Forest Evaluation**
Trained a simple Random Forest model as a reference point.  
Metrics used:
- Accuracy  
- ROC Curve  
- AUC  

---

### **3ï¸âƒ£ Hyperparameter Tuning using RandomizedSearchCV**
Defined a broad search space:

- `n_estimators`  
- `max_depth`  
- `min_samples_split`  
- `min_samples_leaf`  
- `max_features`  

RandomizedSearchCV performed:
- 20 random hyperparameter combinations  
- 5-fold cross-validation  
- Selected the best model  

---

### **4ï¸âƒ£ Tuned Model Evaluation**
Evaluated the optimized model on the test set:

- Higher **accuracy**  
- Improved **AUC**  
- Better **precision**, **recall**, and **F1-score**

---

### **5ï¸âƒ£ ROC Curve Visualization**
Plotted the ROC curve for the tuned Random Forest.  
The curve is significantly above the baseline model, confirming improved performance.

---

### **6ï¸âƒ£ Baseline vs Tuned Model Comparison**

| Model | Accuracy | AUC |
|-------|----------|------|
| Baseline Random Forest | â†‘ baseline_acc | â†‘ baseline_auc |
| **Tuned Random Forest** | **â†‘ improved_acc** | **â†‘ improved_auc** |

*(Values replaced by actual results in notebook)*

The tuned model clearly outperformed the baseline.

---

### **7ï¸âƒ£ Saved Final Optimized Model**
Exported the best model as: 

best_model/tuned_random_forest.pkl

This model can now be reused for inference or deployment.

---

## ğŸ“ Files in This Folder

Day7_Tuning_RandomForest.ipynb

README.md

best_model/
â””â”€â”€ tuned_random_forest.pkl

---

## ğŸ¯ Final Summary

Day 7 completes the Titanic ML pipeline with professional-level model optimization.  
You now understand:

- How to tune machine learning models  
- How to evaluate model improvements  
- How to save and reuse trained models  
- How to compare multiple models systematically  

Your Random Forest model is now **fully optimized and production-ready**.

---