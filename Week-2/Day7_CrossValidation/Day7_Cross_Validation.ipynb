{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904194d9-9873-474e-8201-ef6113169c54",
   "metadata": {},
   "source": [
    "# Day 7 - Cross Validation & Model Stability\n",
    "### Machine Learning Roadmap - Week 2\n",
    "### Author - N Manish Kumar\n",
    "---\n",
    "\n",
    "After training models and applying regularization, the next critical question is:\n",
    "\n",
    "**How reliable is our model’s performance estimate?**\n",
    "\n",
    "A single train–test split can be misleading because results may change\n",
    "depending on how the data is divided.\n",
    "\n",
    "To solve this, we use **Cross-Validation**, which evaluates the model\n",
    "across multiple data splits to estimate how well it generalizes.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Use k-fold cross-validation\n",
    "- Compare stability of different models\n",
    "- Select a model using training data only\n",
    "- Evaluate final performance on an untouched test set\n",
    "\n",
    "Dataset used: **Breast Cancer Dataset (sklearn)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d815440-9ff1-4b2e-b01a-fc0940fb82f5",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f16c48b-2acf-4bab-a1b8-c5193072aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (569, 30)\n",
      "Target distribution:\n",
      " 1    357\n",
      "0    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target distribution:\\n\", y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd31888-941f-4604-99b1-3d6e811ed1a3",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff637c0a-5357-4631-9cfd-821a6c59ac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (455, 30)\n",
      "Test set shape: (114, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f6505-9277-4daf-a999-d2c6961f5ff9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e78c1f-94c6-482e-b04f-83148588006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ece1b-7ee3-4b3f-909a-81fa7180f721",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Define Models for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41a5cab2-df99-492e-87da-cb6f5225343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model (almost no regularization)\n",
    "baseline_model = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    C=1e6,\n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "# L2 Regularization (Ridge) -> l1_ratio = 0\n",
    "l2_model = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    l1_ratio=0.0,\n",
    "    C=1.0,\n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "# L1 Regularization (Lasso) -> l1_ratio = 1\n",
    "l1_model = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    l1_ratio=1.0,\n",
    "    C=1.0,\n",
    "    max_iter=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1879390-1c12-4781-b85b-65f21d337590",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. k-Fold Cross-Validation\n",
    "\n",
    "### What is k-Fold Cross-Validation?\n",
    "\n",
    "In k-fold cross-validation, the training dataset is divided into **k equal parts (folds)**.\n",
    "\n",
    "For **k = 5**, the process is:\n",
    "\n",
    "- Use 4 folds for training\n",
    "- Use the remaining 1 fold for validation\n",
    "- Repeat this process **5 times**, changing the validation fold each time\n",
    "\n",
    "So every data point:\n",
    "- Is used for training **4 times**\n",
    "- Is used for validation **1 time**\n",
    "\n",
    "This produces **5 different validation scores**.\n",
    "\n",
    "### Why do we use k-fold Cross-Validation?\n",
    "\n",
    "A single train–test split can give misleading results depending on how data is split.\n",
    "\n",
    "Cross-validation:\n",
    "- Gives a **more reliable estimate** of model performance\n",
    "- Shows how **stable** the model is across different data splits\n",
    "- Helps in **model selection** without touching the test set\n",
    "\n",
    "### Important Rule\n",
    "\n",
    "Cross-validation must be done **only on training data**.  \n",
    "The test set must remain completely unused until final evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43327667-6e51-4142-9e9c-e3421ace03cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CV scores: [0.93406593 0.95604396 0.96703297 1.         0.94505495]\n",
      "L2 CV scores: [0.96703297 0.97802198 0.96703297 1.         0.98901099]\n",
      "L1 CV scores: [0.95604396 0.97802198 0.96703297 0.98901099 0.98901099]\n"
     ]
    }
   ],
   "source": [
    "base_scores = cross_val_score(baseline_model, X_train_s, y_train, cv=5)\n",
    "l2_scores = cross_val_score(l2_model, X_train_s, y_train, cv=5)\n",
    "l1_scores = cross_val_score(l1_model, X_train_s, y_train, cv=5)\n",
    "\n",
    "print(\"Baseline CV scores:\", base_scores)\n",
    "print(\"L2 CV scores:\", l2_scores)\n",
    "print(\"L1 CV scores:\", l1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba56a25-3c10-4b89-874f-1912425afbf4",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "Each value represents the accuracy on one validation fold, so performance changes slightly\n",
    "depending on how the data is split.\n",
    "\n",
    "For model comparison we focus on:\n",
    "- **Mean accuracy** → overall performance\n",
    "- **Standard deviation** → stability across folds\n",
    "\n",
    "A good model should have high mean accuracy and low variance.\n",
    "\n",
    "Models must be selected using cross-validation on training data,\n",
    "and the test set should be used only once for final evaluation.\n",
    "\n",
    "---\n",
    "## 8. Mean and Standard Deviation of CV Scores\n",
    "\n",
    "After performing k-fold cross-validation, we obtain multiple accuracy values\n",
    "for each model — one from each fold.\n",
    "\n",
    "To compare models properly, we compute:\n",
    "\n",
    "- **Mean accuracy** → average performance across folds  \n",
    "- **Standard deviation (std)** → how stable the model is across different splits\n",
    "\n",
    "These two values together give a better estimate of how the model will perform\n",
    "on unseen data than a single train–test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a81d00b-2b82-4bc8-8966-cb03c5284a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline CV Mean: 0.9604395604395604\n",
      "Baseline CV Std : 0.022627758551619782\n",
      "L2 CV Mean: 0.9802197802197803\n",
      "L2 CV Std : 0.012815278889769896\n",
      "L1 CV Mean: 0.9758241758241759\n",
      "L1 CV Std : 0.01281527888976989\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline CV Mean:\", base_scores.mean())\n",
    "print(\"Baseline CV Std :\", base_scores.std())\n",
    "\n",
    "print(\"L2 CV Mean:\", l2_scores.mean())\n",
    "print(\"L2 CV Std :\", l2_scores.std())\n",
    "\n",
    "print(\"L1 CV Mean:\", l1_scores.mean())\n",
    "print(\"L1 CV Std :\", l1_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc6a2d9-bcea-4fe6-94ee-b0ed12ef5a1c",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Higher **mean accuracy** indicates better overall performance.\n",
    "- Lower **standard deviation** indicates more stable and reliable predictions.\n",
    "\n",
    "If two models have similar mean accuracy, the model with **lower standard deviation**\n",
    "is usually preferred because it is less sensitive to how the data is split.\n",
    "\n",
    "In practice, regularized models (especially L2) often show better stability\n",
    "compared to very flexible baseline models.\n",
    "\n",
    "---\n",
    "## 9. Final Model Evaluation on Test Set\n",
    "\n",
    "After selecting the best model using cross-validation on training data,\n",
    "we now evaluate it on the untouched test set.\n",
    "\n",
    "This test set has not been used in:\n",
    "- Training\n",
    "- Cross-validation\n",
    "- Model selection\n",
    "\n",
    "Therefore, the test accuracy represents how well the model is expected\n",
    "to perform on new, unseen data in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab8e4582-b827-43b2-b24c-99599b92ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy (l2): 0.9824561403508771\n"
     ]
    }
   ],
   "source": [
    "# Train best model on full training data\n",
    "l2_model.fit(X_train_s, y_train)\n",
    "\n",
    "# Evaluate on final test set\n",
    "test_accuracy = l2_model.score(X_test_s, y_test)\n",
    "\n",
    "print(\"Final Test Accuracy (l2):\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0371ba0b-3388-4264-a584-c9b282e35124",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The test accuracy gives the most reliable estimate of model performance\n",
    "because the test set was never seen during training or validation.\n",
    "\n",
    "If the test accuracy is close to the cross-validation mean accuracy,\n",
    "it indicates that the model generalizes well and is not overfitting.\n",
    "\n",
    "This completes the standard ML workflow:\n",
    "\n",
    "Train → Cross-Validate → Select Model → Final Test Evaluation.\n",
    "\n",
    "---\n",
    "# Notebook Summary — Week 2 Day 7\n",
    "\n",
    "In this notebook, we practiced the correct ML evaluation workflow:\n",
    "\n",
    "### What was done\n",
    "- Loaded and inspected the Breast Cancer dataset\n",
    "- Created a hold-out test set for final evaluation\n",
    "- Applied feature scaling using StandardScaler\n",
    "- Compared multiple Logistic Regression models using 5-fold cross-validation\n",
    "- Selected the best model based on stability and mean accuracy\n",
    "- Evaluated final performance on the untouched test set\n",
    "\n",
    "### Key Learnings\n",
    "- Single train–test split can be unreliable\n",
    "- Cross-validation gives a better estimate of generalization performance\n",
    "- Model selection must be done using only training data\n",
    "- Test set should be used only once, after model selection\n",
    "\n",
    "### Final Outcome\n",
    "The selected model showed consistent cross-validation performance and\n",
    "achieved similar accuracy on the test set, indicating good generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
