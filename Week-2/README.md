# ğŸ“˜ Week 2 â€” Supervised Learning & Model Evaluation

This week focused on strengthening the **core supervised machine learning foundations**  
with a strong emphasis on **correct evaluation techniques and model improvement strategies**.

We moved beyond just training models and learned how to:
- Evaluate models properly  
- Control overfitting  
- Interpret model behavior  
- Improve performance using features  
- Select models using cross-validation  

The goal of Week 2 was to build **industry-standard ML evaluation and selection skills**.

---

# ğŸ—‚ï¸ Week 2 Structure

Week-2/
â”œâ”€â”€ Day1_LinearRegression/
â”œâ”€â”€ Day2_LogisticRegression/
â”œâ”€â”€ Day3_ModelEvaluation/
â”œâ”€â”€ Day4_Regularization/
â”œâ”€â”€ Day5_FeatureImportance/
â”œâ”€â”€ Day6_FeatureEngineering/
â””â”€â”€ Day7_CrossValidation/

Each folder contains:
- Jupyter Notebook (`.ipynb`)
- Step-by-step explanations
- Evaluation outputs
- Notebook Summary section

---

# ğŸ“… **Day-by-Day Summary**

---

## **ğŸ“˜ Day 1 â€” Linear Regression**
- Built Linear Regression from sklearn  
- Understood relationship between features and target  
- Interpreted coefficients  
- Evaluated using error metrics  
- Learned baseline regression workflow  

---

## **ğŸ“— Day 2 â€” Logistic Regression**
- Learned binary classification using Logistic Regression  
- Understood sigmoid function and decision boundary  
- Trained classifier on labeled dataset  
- Evaluated predictions and class probabilities  

---

## **ğŸ“™ Day 3 â€” Model Evaluation**
- Performed train/test split  
- Built confusion matrix  
- Calculated:
  - Accuracy  
  - Precision  
  - Recall  
  - F1-score  
- Learned why accuracy alone is misleading  

---

## **ğŸ“• Day 4 â€” Regularization & Overfitting**
- Understood overfitting vs underfitting  
- Studied biasâ€“variance tradeoff  
- Applied:
  - L1 (Lasso) regularization  
  - L2 (Ridge) regularization  
- Observed effect on coefficients and performance  

---

## **ğŸ“Š Day 5 â€” Feature Importance & Interpretability**
- Analyzed model coefficients  
- Ranked most influential features  
- Understood how features affect predictions  
- Practiced basic model interpretability techniques  

---

## **ğŸ› ï¸ Day 6 â€” Feature Engineering**
Improved model performance by:
- Creating new features  
- Combining existing attributes  
- Using domain intuition  
- Comparing performance before and after feature engineering  

---

## **ğŸ“ˆ Day 7 â€” Cross-Validation & Model Selection**
Implemented proper ML evaluation workflow:
- Created hold-out test set  
- Applied k-fold cross-validation on training data  
- Compared models using:
  - Mean accuracy  
  - Standard deviation (stability)  
- Selected best model using CV  
- Evaluated final performance on untouched test set  

---

# ğŸ“ˆ **Key Learning Outcomes (Week 2)**

By the end of Week 2, the following ML skills were developed:

### âœ” Linear & Logistic Regression modeling  
### âœ” Proper evaluation metrics for classification  
### âœ” Biasâ€“variance understanding  
### âœ” Regularization techniques (L1 & L2)  
### âœ” Feature importance and interpretability  
### âœ” Feature engineering strategies  
### âœ” Cross-validation for reliable model selection  
### âœ” Preventing data leakage  
### âœ” Structuring ML experiments in notebooks  

---

# ğŸš€ **Final Outcome of Week 2**

By completing Week 2, I can now:

- Train supervised ML models correctly  
- Evaluate models using appropriate metrics  
- Improve generalization using regularization and features  
- Select models using cross-validation instead of test data  
- Follow professional ML experimentation workflow  

This week forms the **core foundation for all advanced ML topics and projects ahead**.

---

# ğŸ¯ Next Steps (after Week 2)

- Begin **Week 3: Model Optimization & Advanced Evaluation**  
- Learn **Hyperparameter Tuning using GridSearchCV**  
- Study learning curves and biasâ€“variance diagnostics  
- Move toward tree-based ensemble models  

These topics will build directly on the evaluation and modeling skills learned in Week 2.

---