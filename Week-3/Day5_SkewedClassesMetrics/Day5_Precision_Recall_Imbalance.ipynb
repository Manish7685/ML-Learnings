{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88776136-e8f2-4c84-bd13-fe8c693168ed",
   "metadata": {},
   "source": [
    "# Day 5 — Skewed Classes & Evaluation Metrics\n",
    "### Machine Learning Roadmap — Week 3\n",
    "### Author — N Manish Kumar\n",
    "---\n",
    "\n",
    "In many real-world problems, one class is much rarer than the other.\n",
    "Examples include:\n",
    "- Cancer detection\n",
    "- Fraud detection\n",
    "- Spam filtering\n",
    "\n",
    "In such cases, a model can achieve very high accuracy by simply predicting\n",
    "the majority class, while still failing to detect important minority cases.\n",
    "\n",
    "Therefore, accuracy alone is not a reliable metric for evaluating models\n",
    "on imbalanced datasets.\n",
    "\n",
    "In this notebook, we will:\n",
    "- Create an imbalanced classification dataset\n",
    "- Train a classifier and observe misleading accuracy\n",
    "- Compute Precision, Recall, and F1-score\n",
    "- Understand which metric is more important for different applications\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Create an Imbalanced Dataset and Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0aa756c6-555c-490c-becc-5c8f939ba0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in full dataset: [1791  209]\n",
      "Class distribution in training set: [1433  167]\n",
      "Class distribution in test set: [358  42]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Create imbalanced dataset (90% of class 0, 10% of class 1)\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    weights=[0.9, 0.1],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Class distribution in full dataset:\", np.bincount(y))\n",
    "print(\"Class distribution in training set:\", np.bincount(y_train))\n",
    "print(\"Class distribution in test set:\", np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d483e2d-3665-4e05-bf11-4530f002146f",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Train Classifier and Evaluate Accuracy\n",
    "\n",
    "We now train a simple Logistic Regression classifier on the imbalanced\n",
    "training data and evaluate it using accuracy.\n",
    "\n",
    "Accuracy measures the percentage of correct predictions, but it does not\n",
    "distinguish between different types of errors.\n",
    "\n",
    "On imbalanced datasets, accuracy can appear high even when the model\n",
    "performs poorly on the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e068a9c5-b855-44cd-b017-be4d6b217a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.895\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703aed4-905c-47ce-86dc-7623fbcdaaab",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Evaluate Precision, Recall, and F1-Score\n",
    "\n",
    "Accuracy alone does not tell us how well the model detects minority-class\n",
    "samples.\n",
    "\n",
    "We therefore compute:\n",
    "- Precision: How many predicted positives are actually positive\n",
    "- Recall: How many actual positives were correctly detected\n",
    "- F1-score: Balance between precision and recall\n",
    "\n",
    "These metrics give a clearer picture of model performance on imbalanced data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73d5902c-ded9-47f3-9fa9-84544968be00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5\n",
      "Recall: 0.047619047619047616\n",
      "F1-score: 0.08695652173913043\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0baa92-33db-4dc9-b9ba-93ee0c0787c3",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Precision indicates how reliable positive predictions are, while recall\n",
    "indicates how many actual positive cases the model successfully detected.\n",
    "\n",
    "On imbalanced datasets, recall is often low, meaning the model misses many\n",
    "minority-class cases even if accuracy is high.\n",
    "\n",
    "F1-score summarizes the balance between precision and recall and is often\n",
    "more informative than accuracy in such scenarios.\n",
    "\n",
    "---\n",
    "## 5. Compare with Dummy Baseline (Always Predict Majority Class)\n",
    "\n",
    "To understand how misleading accuracy can be, we compare our trained model\n",
    "with a very simple baseline that always predicts the majority class.\n",
    "\n",
    "If this dummy model achieves similar accuracy, it means accuracy alone\n",
    "is not a useful metric for this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38411d3b-e4ce-473e-b541-8933bb4db674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Accuracy: 0.895\n",
      "Dummy Precision: 0.0\n",
      "Dummy Recall: 0.0\n",
      "Dummy F1: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Dummy model that always predicts the most frequent class\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "\n",
    "y_dummy_pred = dummy.predict(X_test)\n",
    "\n",
    "dummy_acc = accuracy_score(y_test, y_dummy_pred)\n",
    "dummy_precision = precision_score(y_test, y_dummy_pred, zero_division=0)\n",
    "dummy_recall = recall_score(y_test, y_dummy_pred, zero_division=0)\n",
    "dummy_f1 = f1_score(y_test, y_dummy_pred, zero_division=0)\n",
    "\n",
    "print(\"Dummy Accuracy:\", dummy_acc)\n",
    "print(\"Dummy Precision:\", dummy_precision)\n",
    "print(\"Dummy Recall:\", dummy_recall)\n",
    "print(\"Dummy F1:\", dummy_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547243b6-0d9f-4189-8efb-b105271317d1",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The dummy model may achieve high accuracy by predicting only the majority class,\n",
    "but its recall for the minority class is zero, meaning it completely fails to\n",
    "detect rare cases.\n",
    "\n",
    "If our trained model shows only slightly better accuracy than the dummy model,\n",
    "it confirms that accuracy is not sufficient for evaluating performance on\n",
    "imbalanced datasets.\n",
    "\n",
    "This highlights the importance of using precision, recall, and F1-score.\n",
    "\n",
    "---\n",
    "## 6. Choosing the Right Metric Based on Application\n",
    "\n",
    "Different applications care about different types of errors.\n",
    "\n",
    "Examples:\n",
    "- Medical diagnosis → Missing a positive case is very dangerous → prioritize Recall\n",
    "- Spam detection → False alarms are annoying → prioritize Precision\n",
    "- Balanced importance → Use F1-score\n",
    "\n",
    "Therefore, model evaluation should focus on the metric that aligns with\n",
    "real-world costs of mistakes, not just accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d2cea87-093f-4f68-8d75-380f4125e0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dummy Baseline</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall  F1-score\n",
       "0  Logistic Regression     0.895        0.5  0.047619  0.086957\n",
       "1       Dummy Baseline     0.895        0.0  0.000000  0.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"Dummy Baseline\"],\n",
    "    \"Accuracy\": [acc, dummy_acc],\n",
    "    \"Precision\": [precision, dummy_precision],\n",
    "    \"Recall\": [recall, dummy_recall],\n",
    "    \"F1-score\": [f1, dummy_f1]\n",
    "})\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d773662-7aa5-4512-aa53-07d48e1a2323",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Comparing all metrics together shows that:\n",
    "- Accuracy alone may not differ much between models\n",
    "- Recall and F1-score reveal large differences in real usefulness\n",
    "\n",
    "For problems where detecting rare events is critical, recall (or F1-score)\n",
    "should be the primary metric used for model selection and tuning.\n",
    "\n",
    "---\n",
    "# Notebook Summary — Week 3 Day 5\n",
    "\n",
    "In this notebook, we studied why accuracy can be misleading when working with\n",
    "imbalanced classification datasets and learned to use better evaluation metrics.\n",
    "\n",
    "### What was done\n",
    "- Created an intentionally imbalanced dataset\n",
    "- Trained a Logistic Regression classifier\n",
    "- Evaluated performance using accuracy\n",
    "- Computed Precision, Recall, and F1-score\n",
    "- Compared results with a dummy baseline model\n",
    "- Compared all metrics side-by-side to guide model evaluation\n",
    "\n",
    "### Key Learnings\n",
    "- High accuracy does not guarantee good performance on minority classes\n",
    "- Recall measures how many real positive cases are detected\n",
    "- Precision measures how reliable positive predictions are\n",
    "- F1-score balances precision and recall\n",
    "- Metric selection must depend on the real-world cost of mistakes\n",
    "\n",
    "### Final Outcome\n",
    "The analysis showed that models must be evaluated using metrics aligned with\n",
    "the problem objective, and that recall or F1-score is often more important than\n",
    "accuracy in imbalanced datasets.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
